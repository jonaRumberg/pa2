\section{Theoretischer Hintergrund}
\subsection{Maschinelles Lernen}
Maschinelles Lernen beschreibt das Konzept, auf Basis von einer großen Menge von Daten Algorithmen zu approximieren, die auf anderem Wege nicht erschlossen werden können. Man nehme beispielsweise die klassische Aufgabe, ein Programm zu schreiben, das in der Lage ist, Bilder von Hunden und Katzen zu unterscheiden. Wenn wir als Menschen uns dieser Aufgabe stellen, müssen wir nicht lange überlegen, wir lösen sie intuitiv. Wenn wir uns aber fragen, nach welchen Regeln wir diese Entscheidung treffen, wird es schon schwieriger. Wir könnten uns auf die Form der Ohren, die Farbe des Fells oder die Größe des Tieres konzentrieren. Aber wie genau wir Regelmäßigkeiten definieren, ist nicht so einfach. Maschinelles Lernen verfolgt den Ansatz, genau solche Regeln nicht mehr fest zu definieren, sondern sie anhand von einer großen Menge von Daten zu lernen \parencite[Vgl.][S. 2f.]{bishop_2006}.

\subsection{Neuronale Netze}
Ein Mittel der Wahl um das Konzept des maschinellen Lernens umzusetzen, sind sogenannte künstliche Neuronale Netze. Neuronale Netze, lose inspiriert von der Struktur des menschlichen Gehirns, bestehen aus einer Vielzahl von einfacher Einheiten, sogenannte Knoten, die in Schichten angeordnet sind und über unterschiedlich gewichtete Verbindungen verknüpft sind. Diese Struktur ermöglicht es, komplexe statistische Zusammenhänge in einem Datensatz zu modellieren, indem für einen gegebenen Datensatz mithilfe von Techniken des maschinellen Lernens die Parameter, also beispielsweise die Gewichte der Verbindungen des Netzes, so angepasst werden, dass sie die gegeben Daten möglichst genau abbilden. Wurde dieser Prozess erfolgreich durchlaufen, so kann das Modell im Anschluss dazu genutzt werden, Aussagen über Daten, die es im Lernprozess noch nie gesehen hat, zu treffen oder Vorhersagen abzugeben. Das interessante an diesem Ansatz ist es, dass durch diesen Ansatz, gerade bei großen neuronalen Netzen auch nicht triviale, subtile Muster im Datensatz erkannt werden können und so, wie oben bereits angedeutet, Approximationen für Probleme getroffen werden können, die formal nur schwer beschrieben werden können \parencite[Vgl.][S. 225f.]{bishop_2006}.

\subsection{Word Embeddings}
Eine weitere, für diese Arbeit relevante Entwicklung der jüngeren Forschung sind die Fortschritte der Computerlinguistik. Ein Kernproblem dieses Feldes ist die Forschung an der Repräsentationen von Sprache. Hierbei geht es nicht einfach darum, einzelne Wörter in ihrer Schriftform zu speichern, sondern vielmehr den Wort\emph{sinn} festzuhalten. Man betrachte zum Beispiel die Wörter \emph{Couch} und \emph{Sofa}, die in Schriftform, mit Ausnahme des zweiten Buchstabens vollkommen unterschiedlich sind, in ihrer Bedeutung aber nahezu Synonym verwendet werden. Weiterhin möchten wir Aussagen über die Beziehung von Wörtern treffen können. \emph{Heiß} und \emph{kalt} haben in ihrer Wortbedeutung einen klaren Zusammenhang (Es handelt sich um Gegensätze), den wir eventuell darstellen möchten, genauso wie \emph{Replika} und \emph{Fälschung} im Grunde dasselbe meinen, aber einen klaren Unterschied in ihrer Konnotation aufweisen \parencite[Vgl.][S. 106ff.]{jurafsky_martin_2020}. 
Eine Form der semantisch reichen Repräsentationen zu finden, die diesen Anforderungen genügt ist nicht trivial, es handelt sich aber wieder um ein solches Problem, das, wie oben beschrieben, intuitiv einfach zu lösen, formal jedoch schwer zu beschreiben ist. Und genau wie oben beschrieben, können die Techniken aus dem Feld des maschinellen Lernens auf dieses Problem angewandt werden, um es zu lösen. \\

Die Grundlage für die nun folgenden Überlegungen bildet die 1950 erstmals formulierte Verteilungshypothese der Linguistik. Im Grunde besagt sie, dass Wörter, die in ähnlichen Kontexten auftauchen, eine ähnliche Bedeutung haben. Wenn die Wörter \emph{Pizza} und \emph{Burger} beispielsweise beide häufig im Zusammenhang mit den Wörtern \emph{Essen} und \emph{geniessen} auftauchen, kann daraus geschlossen werden, dass sie ihr Wortsinn eine Ähnlichkeit hat, in diesem Fall, dass es sich bei beiden Wörtern um Essen handelt. \parencite[Vgl.][S. 109]{jurafsky_martin_2020}\\

Auf Basis dieser Erkenntnis kann eine erste simple Repräsentationen des Wortsinns gefunden werden. Gegeben sei ein Corpus $C$ auf Basis dessen wir einen Wortsinn für jedes Wort im Vokabular $V$ des Corpus finden wollen. Auf Basis der Verteilungshypothese kann nun eine Wort-Wort-Matrix aufgestellt werden, die abbildet, wie häufig Worte im Kontext anderer Worte auftauchen. Dafür muss ein Kontext definiert werden, häufig ist dieser Kontext ein Bereich um das Wort, kann aber auch beliebig definiert werden, beispielsweise als eine Menge Dokumente im Corpus. Als Ergebnis erhält man eine Matrix mit der Dimension $ | V | \times | V | $, beziehungsweise einen $|V|$-dimensionalen Spaltenvektor für jedes Wort \parencite[Vgl. ][S. 113]{jurafsky_martin_2020}.\\

\begin{table}
    \centering
    \begin{tabular}{ | c | c | c | c | }
        \rowcolor{tableHeading} & Essen & italienisch & Auto \\
        Pizza & 150 & 122 & 11 \\
        Burger & 136 & 3 & 13 \\
        Porsche & 0 & 6 & 350 \\
        Ferrari & 1 & 199 & 475 \\
    \end{tabular}
    \label{Vector Semantics Word-Word Vec Table}
    \caption{Wort-Wort-Matrix auf Basis des Wikipedia Corpus und ausgewählten Worten \parencite{davies_2015}}
\end{table}
\begin{figure}
    \tdplotsetmaincoords{70}{110}
    \begin{tikzpicture}[tdplot_main_coords,line cap=round,>=stealth]
    \draw (0,0,0) --  (14,0,0)  node[pos=1.1]{Auto};
    \draw (0,0,0) -- (0,6,0) node[pos=1.105]{Essen};
    \draw (0,0,0) -- (0,0,6) node[pos=1.05]{italienisch};

    \draw  (5,0,0) node[right]{$150$} -- ++ (0,0,0.15)
            (0,5,0) node[below]{$150$} -- ++ (0,0,0.15)
            (0,0,5) node[right]{$150$} -- ++ (0,-0.15,0);

    \draw[very thick,->=0.7, color=blue] (0,0,0)  -- (0.36,5,4.03) node[above]{Pizza};
    \draw[very thick,->=0.7, color=blue] (0,0,0)  -- (0.43,4.53,0.1) node[above]{Burger}; 
    \draw[very thick,->=0.7, color=blue] (0,0,0)  -- (11.66,0,0.2) node[left]{Porsche};
    \draw[very thick,->=0.7, color=blue] (0,0,0)  -- (15.83,0.03,6.63) node[above]{Ferrari};

    \end{tikzpicture}
    \caption{Visualisierung der in Tabelle \ref{Vector Semantics Word-Word Vec Table} berechneten Vektoren in dreidimsionalem Raum}
    \label{Vector Semantics Word-Word Vec}
\end{figure}
Die so gewonnen Vektoren haben bereits einige der gewünschte Eigenschaften um den Wortsinn zu encodieren. Die in Tabelle \ref{Vector Semantics Word-Word Vec Table} dargestellte Wort-Wort-Matrix basiert auf dem Corpus der englischsprachigen Wikipedia und illustriert eine Eigenschaft, die auch noch bei der Reduktion auf wenige Dimensionen sichtbar wird: Wörter, die sich ähnlich sind, tauchen in ähnlichen Kontexten auf. Die Vektoren für Automarken tauchen beide ähnlich häufig im Kontext des Wortes Auto auf, genauso wie Essbares ähnlich häufig im Kontext von dem Wort Essen auftauchen. Auch eine 1970 entwicklelte Methode zur Beschreibung von Analogieproblemen lässt sich anwenden. \parencite[Vgl.][]{RUMELHART19731} Die Idee dieser Methode ist es Fragen nach folgendem Schema zu stellen: \emph{Deutschland gehört zu Berlin. Was gehört zu Paris?} Auch bei diesem sehr simplifizierten Beispiel lässt sich ein solches Sinn-Parallelogram aufstellen. Anhand der in Abbildung \ref{Vector Semantics Word-Word Vec} getroffenen Visualisierung des Beispiels lässt sich leicht erkennen, dass dieses Problem durch einfach Vektorrechnung lösen lässt. \emph{Pizza gehört zu Burger. Was gehört zu Porsche?}. Die Antwort innerhalb dieses Beispiels ist Ferrari und lässt sich bestimmen, indem der Vektor für Pizza von Burger subtrahiert wird und das Ergebnis dieser Operation auf den Vektor für Porsche aufaddiert wird. Das Ergebnis ist ein Vektor der in die Nähe von Ferrari zeigt. Dieses Beispiel ist selbstverständlich enorm simplifiziert, es lässt sich aber genau diese Encodierung von Wortsinn auch bei größeren Vokabularien feststellen\footnote{Einer der wohl bekanntesten Beispiele aus diesem Feld ist der Erfolg aus dem word2vec paper, indem das Modell den Zusammenhang zwischen König minus Mann plus Frau ist gleich Königin herstellen konnte \parencite[Vgl.][]{mikolov_chen_corrado_dean_2013}.} \parencite[Vgl.][S. 128f.]{jurafsky_martin_2020}. \\

Vorgestellt wurde die hiermit die einfachste Form von statischen Embeddings, mittlerweile gibt es eine Vielzahl von Techniken um dieses stumpfe Zählen von Worten mit verschiedenen Verfahren zu optimieren und schlussendlich bessere und sinnhaltigere Ergebnisse zu erzielen. Ein Problem dieser Methode ist beispielsweise, dass die so gewonnen Vektoren eine sehr hohe Dimensionalität haben ($|V|$), dafür aber größtenteils leer sind, also 0 enthalten. Die Empirik hat gezeigt, dass sich wesentlich bessere Ergebnisse erzielen lassen, wenn Wörter als Vektoren mit niedrigerer Dimensionalität dargestellt werden. Die Intuition hinter dieser Erkenntnis ist es, dass dadurch eine gewisse "Abstraktion" des Wortsinnes stattfindet und so konzeptuelle Zusammenhänge besser abgebildet werden können. \parencite[Vgl.][S. 113f.]{jurafsky_martin_2020} Eine Methode, diese Reduktion der Dimensionalität mithilfe der oben besprochenen Konzepte des maschinellen Lernens zu erreichen, soll im Folgenden dargestellt werden.